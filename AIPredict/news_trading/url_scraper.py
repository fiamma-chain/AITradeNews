"""
URLå†…å®¹çˆ¬å–å·¥å…·
URL Content Scraper for User-Submitted News
"""
import logging
import httpx
from bs4 import BeautifulSoup
from typing import Optional

logger = logging.getLogger(__name__)


async def scrape_url_content(url: str) -> Optional[str]:
    """
    çˆ¬å–URLå†…å®¹
    
    Args:
        url: ç›®æ ‡URL
        
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        logger.info(f"ğŸŒ å¼€å§‹çˆ¬å–URL: {url}")
        
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers={
                "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36"
            })
            
            if response.status_code != 200:
                logger.error(f"âŒ URLè¿”å›å¼‚å¸¸çŠ¶æ€ç : {response.status_code}")
                return None
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤scriptå’Œstyleæ ‡ç­¾
            for script in soup(["script", "style", "meta", "link"]):
                script.decompose()
            
            # æå–æ–‡æœ¬
            text = soup.get_text(separator=' ', strip=True)
            
            # æ¸…ç†å¤šä½™ç©ºç™½
            lines = [line.strip() for line in text.split('\n')]
            text = ' '.join([line for line in lines if line])
            
            # é™åˆ¶é•¿åº¦ï¼ˆé˜²æ­¢è¿‡é•¿ï¼‰
            if len(text) > 5000:
                text = text[:5000] + "..."
            
            logger.info(f"âœ… æˆåŠŸçˆ¬å–å†…å®¹ï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
            return text
    
    except httpx.TimeoutException:
        logger.error(f"âŒ çˆ¬å–URLè¶…æ—¶: {url}")
        return None
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–URLå¤±è´¥: {e}")
        return None

