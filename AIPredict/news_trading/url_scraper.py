"""
URLå†…å®¹çˆ¬å–å·¥å…·
URL Content Scraper for User-Submitted News
"""
import logging
import httpx
from bs4 import BeautifulSoup
from typing import Optional

logger = logging.getLogger(__name__)


async def scrape_url_content(url: str) -> Optional[str]:
    """
    çˆ¬å–URLå†…å®¹ï¼Œå¦‚æœçˆ¬å–å¤±è´¥åˆ™ä»URLæ¨æ–­ä¿¡æ¯
    
    Args:
        url: ç›®æ ‡URL
        
    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹ï¼Œå¤±è´¥è¿”å›None
    """
    try:
        logger.info(f"ğŸŒ å¼€å§‹çˆ¬å–URL: {url}")
        
        # å¢å¼ºçš„headersï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
        headers = {
            "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.9",
            "Accept-Encoding": "gzip, deflate, br",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
            "Sec-Fetch-Dest": "document",
            "Sec-Fetch-Mode": "navigate",
            "Sec-Fetch-Site": "none",
            "Cache-Control": "max-age=0"
        }
        
        async with httpx.AsyncClient(timeout=15.0, follow_redirects=True) as client:
            response = await client.get(url, headers=headers)
            
            # æ¥å—200å’Œ202çŠ¶æ€ç 
            if response.status_code not in [200, 202]:
                logger.warning(f"âš ï¸  URLè¿”å›çŠ¶æ€ç  {response.status_code}ï¼Œå°è¯•ä»URLæ¨æ–­")
                return _infer_from_url(url)
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯WAFæŒ‘æˆ˜é¡µé¢
            if len(response.text) < 5000 and ('gokuProps' in response.text or 'challenge' in response.text.lower()):
                logger.warning("âš ï¸  æ£€æµ‹åˆ°WAFä¿æŠ¤ï¼Œå°è¯•ä»URLæ¨æ–­")
                return _infer_from_url(url)
            
            # ä¼˜å…ˆæå–æ ‡é¢˜
            title = None
            
            # å°è¯•å¤šç§æ ‡é¢˜æå–æ–¹æ³•
            title_selectors = [
                # Metaæ ‡ç­¾ï¼ˆæœ€å¯é ï¼‰
                ('meta', {'property': 'og:title'}),
                ('meta', {'name': 'twitter:title'}),
                ('meta', {'name': 'title'}),
                # HTMLæ ‡ç­¾
                ('h1', {}),
                ('title', {}),
            ]
            
            for tag_name, attrs in title_selectors:
                if tag_name == 'meta':
                    tag = soup.find(tag_name, attrs)
                    if tag and tag.get('content'):
                        title = tag.get('content').strip()
                        if title:
                            break
                else:
                    tag = soup.find(tag_name, attrs)
                    if tag:
                        title = tag.get_text(strip=True)
                        if title and len(title) > 10:  # ç¡®ä¿æ ‡é¢˜æœ‰æ„ä¹‰
                            break
            
            # å¦‚æœæ‰¾åˆ°æ ‡é¢˜
            if title:
                logger.info(f"âœ… æˆåŠŸæå–æ ‡é¢˜: {title}")
                
                # å°è¯•æå–æè¿°
                description = None
                desc_selectors = [
                    ('meta', {'name': 'description'}),
                    ('meta', {'property': 'og:description'}),
                    ('meta', {'name': 'twitter:description'}),
                ]
                
                for tag_name, attrs in desc_selectors:
                    tag = soup.find(tag_name, attrs)
                    if tag and tag.get('content'):
                        description = tag.get('content').strip()[:500]
                        if description:
                            break
                
                # ç»„åˆæ ‡é¢˜å’Œæè¿°
                if description:
                    content = f"{title}. {description}"
                else:
                    content = title
                
                logger.info(f"âœ… å†…å®¹é•¿åº¦: {len(content)} å­—ç¬¦")
                return content
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä»URLæ¨æ–­
            logger.warning("âš ï¸  æœªæ‰¾åˆ°æ ‡é¢˜ï¼Œå°è¯•ä»URLæ¨æ–­")
            return _infer_from_url(url)
    
    except httpx.TimeoutException:
        logger.error(f"âŒ çˆ¬å–URLè¶…æ—¶ï¼Œå°è¯•ä»URLæ¨æ–­")
        return _infer_from_url(url)
    except Exception as e:
        logger.error(f"âŒ çˆ¬å–URLå¤±è´¥: {e}ï¼Œå°è¯•ä»URLæ¨æ–­")
        return _infer_from_url(url)


def _infer_from_url(url: str) -> Optional[str]:
    """
    ä»URLæ¨æ–­å†…å®¹ï¼ˆå½“çˆ¬å–å¤±è´¥æ—¶çš„å›é€€æ–¹æ¡ˆï¼‰
    
    Args:
        url: URLå­—ç¬¦ä¸²
        
    Returns:
        æ¨æ–­çš„å†…å®¹æè¿°
    """
    try:
        logger.info(f"ğŸ” ä»URLæ¨æ–­å†…å®¹: {url}")
        
        url_lower = url.lower()
        
        # è¯†åˆ«äº¤æ˜“æ‰€
        exchange = None
        if 'binance' in url_lower:
            exchange = 'Binance'
        elif 'coinbase' in url_lower:
            exchange = 'Coinbase'
        elif 'upbit' in url_lower:
            exchange = 'Upbit'
        elif 'bybit' in url_lower:
            exchange = 'Bybit'
        elif 'okx' in url_lower or 'okex' in url_lower:
            exchange = 'OKX'
        elif 'kraken' in url_lower:
            exchange = 'Kraken'
        elif 'kucoin' in url_lower:
            exchange = 'KuCoin'
        
        # è¯†åˆ«äº‹ä»¶ç±»å‹ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰
        event_type = None
        
        # ä¼˜å…ˆæ£€æŸ¥ç»„åˆå…³é”®è¯
        if 'will-list' in url_lower or 'will_list' in url_lower or 'willlist' in url_lower:
            event_type = 'will list'
        elif 'new-listing' in url_lower or 'new_listing' in url_lower:
            event_type = 'new listing'
        elif 'list' in url_lower or 'listing' in url_lower:
            event_type = 'listing'
        elif 'launch' in url_lower:
            event_type = 'launch'
        elif 'delist' in url_lower:
            event_type = 'delisting'
        
        # è¯†åˆ«å¸‚åœºç±»å‹
        market_type = None
        if 'futures' in url_lower or 'perpetual' in url_lower:
            market_type = 'futures'
        elif 'spot' in url_lower:
            market_type = 'spot'
        
        # æ„é€ æè¿°
        parts = []
        if exchange:
            parts.append(exchange)
        
        # ç»„åˆå¸‚åœºç±»å‹å’Œäº‹ä»¶ç±»å‹
        if market_type and event_type:
            parts.append(f'{market_type} {event_type}')
        elif event_type:
            parts.append(event_type)
        elif market_type:
            parts.append(f'{market_type} announcement')
        else:
            # å¦‚æœåœ¨announcementè·¯å¾„ä¸‹ï¼Œæ¨æµ‹æ˜¯å…¬å‘Š
            if 'announcement' in url_lower or 'support' in url_lower:
                # å°è¯•ä»URLæ¨æµ‹æ˜¯listingç›¸å…³
                if exchange and ('detail' in url_lower or 'article' in url_lower):
                    parts.append('listing announcement')
                else:
                    parts.append('announcement')
            else:
                parts.append('announcement')
        
        description = ' '.join(parts) if parts else 'Cryptocurrency announcement'
        
        logger.info(f"âœ… æ¨æ–­å†…å®¹: {description}")
        return description
    
    except Exception as e:
        logger.error(f"âŒ URLæ¨æ–­å¤±è´¥: {e}")
        return "Cryptocurrency listing announcement"

